from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from DPFunctions import *

import time
import re
import json
import sqlite3
import traceback
import os

# specify the URL you want to scrape
account_name = '_realvintage'
url = f'https://www.depop.com/{account_name}/'

chrome_options = Options()
chrome_options.add_experimental_option("detach", True)
driver = webdriver.Chrome("D:\\Selenium_python2\\chromedriver.exe", chrome_options=chrome_options)
actions = ActionChains(driver)
# driver.maximize_window()
driver.get(url)

depop_brand_list = []

def picking_brand(dabrand):
    try:
        brand_down = driver.find_element('xpath', '//*[@id="listingBrands__select"]')
        brand_down.send_keys(dabrand, Keys.ENTER)
        depop_brand_list.append(dabrand)
        save_changes_button = driver.find_element('xpath', '//*[contains(@class, "styles__SaveButton-sc")]')
        driver.execute_script("arguments[0].scrollIntoView(true)", save_changes_button)
        time.sleep(1)
        save_changes_button.click()
        time.sleep(3)
    except Exception as e:
        print(e)
        traceback.print_exc()
        pass

def each_post():
    """ 
    where we parse through each post to find the information available to it
    """
    total_available_post = driver.find_elements('css selector', '[data-testid="product__item"]')
    for post_id, ind_post in enumerate(total_available_post):
        # if post_id < 931:
        #     continue
        driver.execute_script("arguments[0].scrollIntoView(true)", ind_post)
        post_url = ind_post.get_attribute('href')
        print(' ')
        print(f'next post {post_id}')
        driver.execute_script(f"window.open('{post_url}');")
        driver.switch_to.window(driver.window_handles[-1])
        try:
            title_parsing()
        except Exception as e:
            print(e)
            traceback.print_exc()
            driver.close()
            driver.switch_to.window(driver.window_handles[0])

depop_title_list = []
all_titles = []
def hashtag_parser(word):
    """ 
    breaks down the title of the item
    """
    global title_of_item
    ind_title = []
    # time.sleep(2)
    #! need to figure out how to identify the description at all time using this
    #? maybe finding a group of ------------------------------- regardless of what the description is
    word = word.split("PLEASE READ CAREFULLY!! Some items may have unlisted markings, but for the most part all marking would be listed. These clothes are often old and used, so tend to not be in pristine condition. Most items that are listed here will by default be used, unless stated otherwise. Items have not been washed so we also highly recommend that you wash before putting it on.  All items have been handpicked by us. Thank you for choosing to support us at RealVintage._ and if there is anything we can do to make your experience better feel free to send us a message! We also give large discounts if purchasing stuff in bulk.")
    for letter in word:
        if letter != '#' or ',' or '\n' or "PLEASE":
            ind_title.append(letter)
        else:
            overall_title = ''.join(ind_title)
            print(overall_title)

            if overall_title not in all_titles:
                all_titles.append(overall_title)
            break
    title =''.join(ind_title)
    item_title = title.split('#', maxsplit=1)[0]
    title_of_item = item_title.replace('\n', ' ')
    try:
        title_of_item = title_of_item.encode('ascii', 'ignore').decode('unicode_escape')
        depop_title_list.append(title_of_item)
    except:
        depop_title_list.append(title_of_item)
    print(title_of_item)
        
depop_men_or_wmn_list = []
depop_gen_category_list = []
depop_sub_category_list = []
depop_size_list = []
depop_condition_list = []
depop_images_list = []
depop_image_len_list = []
depop_price_list = []
depop_style_list = []
depop_color1_list = []
depop_color2_list = []
depop_likes_list = []
depop_bag_list = []
depop_boosted_list = []
depop_url_list = []
def title_parsing():
    global title_of_item
    time.sleep(1)
    script_tag = driver.find_element('css selector', "[data-testid='meta-schema__json-ld']")
    script_content = script_tag.get_attribute("innerHTML")
    data = script_content.encode('ascii', 'ignore').decode('unicode_escape')
    data = json.loads(script_content)
    
    #price
    price = data['offers']['price']
    if price == '99999.00':
        driver.close()
        driver.switch_to.window(driver.window_handles[0])
        time.sleep(2)
        return
    else:
        depop_price_list.append(price)
        
        #url
        url_string = data['offers']['url']
        depop_url_list.append(str(url_string))

    # Extract the item title description
        item_title = data["description"]
        hashtag_parser(item_title)
        
        #images
        images = data['image']
        depop_images_list.append(images)
        depop_image_len_list.append(int(len(images)))
        
        #condition
        found_size = False
        found_condition = False
        condition_list = ['Brand new', 'Like new', 'Used - Excellent', 'Used - Good', 'Used - Fair']
        size_and_condition = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located(('xpath', "//*[contains(@class, 'ProductAttributes-styles__Attribute-sc')]")))
        for sac in size_and_condition:
            if 'Size' in sac.text:
                found_size = True
                depop_size_list.append(sac.text.replace('Size ', ''))
            if sac.text in condition_list:
                found_condition = True
                depop_condition_list.append(sac.text)
        if not found_condition:
            depop_condition_list.append('Used - Excellent')
        if not found_size:
            depop_size_list.append(' ')

        #brand
        found_brand = False
        total_breadcrumbs = driver.find_elements('css selector', "[data-testid='breadcrumb__item--link']")
        for breadcrumb_id, breadcrumbs in enumerate(total_breadcrumbs):
            if 'Brand' in breadcrumbs.text:
                found_brand = True
                brand_text = total_breadcrumbs[breadcrumb_id + 1].find_element('xpath', './/*[@itemprop="name"]').text
                depop_brand_list.append(brand_text)
                break
        if not found_brand:
            depop_brand_list.append(' ')

        try:
            extracted_color_list = []
            color_list = ['Black', 'Grey', 'White', 'Brown', 'Tan', 'Cream', 'Yellow', 'Red', 'Burgundy', 'Orange', 'Pink', 'Purple', 'Blue', 'Navy', 'Green', 'Khaki', 'Multi', 'Silver', 'Gold']
            color_elements = driver.find_elements('xpath', '//*[contains(@class, "DetailText")]')
            for colors in color_elements:
                if colors.text in color_list:
                    extracted_color_list.append(colors.text)
            color1 = extracted_color_list[0]
            color2 = extracted_color_list[1] if len(extracted_color_list) > 1 else ''
            depop_color1_list.append(color1)
            depop_color2_list.append(color2)

        except Exception as e:
            print(e)
            depop_color1_list.append('')
            depop_color2_list.append('')
            
        # specific category
        category = driver.find_elements('xpath', '//*[contains(@class, "BreadcrumbLink")]')
        try:
            men_or_wmn_text = category[3].text
            if men_or_wmn_text in ['Menswear', 'Womenswear', 'Kids']:
                depop_men_or_wmn_list.append(men_or_wmn_text)
                index = 3
            elif category[1].text in ['Menswear', 'Womenswear', 'Kids']:
                men_or_wmn_text = category[1].text
                depop_men_or_wmn_list.append(men_or_wmn_text)
                index = 1
        except:
            depop_men_or_wmn_list.append('')

        #general category
        try:
            gen_category_text = category[index + 1].text.replace('swear', '')
            depop_gen_category_list.append(gen_category_text)
        except:
            depop_gen_category_list.append(' ')
        try:

            sub_category_text = category[index + 2].text
            depop_sub_category_list.append(sub_category_text)
        except:
            depop_sub_category_list.append(' ')  

        all_text = driver.find_element('xpath', "/html/body").text
        bag_match = re.search(r'\d{1,3}$', all_text)
        if bag_match:
            number = int(bag_match.group())
            depop_bag_list.append(number)
        else:
            depop_bag_list.append(0)

        #total likes
        try:
            like_count = driver.find_elements('css selector', '[data-testid="like-count"]')
            depop_likes_list.append(like_count[0].text.replace(' likes', ''))
        except IndexError:
            depop_likes_list.append('0')

        if 'Unboost listing' in all_text:
           depop_boosted_list.append(1) # 1 = boosted
        else:
            depop_boosted_list.append(0) # 2 = Not boosted

        driver.close()
        driver.switch_to.window(driver.window_handles[0])
        time.sleep(2)

def checking_all_match():
    """
    checks if all lists are matching in length
    """
    print(account_name)
    if len(depop_men_or_wmn_list) == len(depop_title_list) and len(depop_boosted_list) and len(depop_gen_category_list) and len(depop_sub_category_list) and len(depop_condition_list) and len(depop_brand_list) and len(depop_images_list) and len(depop_image_len_list) and len(depop_price_list) and len(depop_color1_list) and len(depop_color2_list) and len(depop_likes_list) and len(depop_bag_list) and len(depop_url_list):
        print('This is all good')
        print('this the max count', len(depop_title_list))

    else:
        print('Fix this')
        print('depop total titles ' + str(len(depop_title_list)))
        print('depop men or women ' + str(len(depop_men_or_wmn_list)))
        print('depop gen category list ' + str(len(depop_gen_category_list)))
        print('depop sub category_list ' + str(len(depop_sub_category_list)))
        print('depop boosted list ' + str(len(depop_boosted_list)))
        print('depop brand_list ' + str(len(depop_brand_list)))
        print('depop url list'+ str(len(depop_url_list)))
        print('depop_condition_list ' + str(len(depop_condition_list)))
        print('depop image_list ' + str(len(depop_images_list)))
        print('depop image len_lists ' + str(len(depop_image_len_list)))
        print('depop price list ' + str(len(depop_price_list)))
        print('depop color1 list ' + str(len(depop_color1_list)))
        print('depop color2 list ' + str(len(depop_color2_list)))
        print('depop likes list ' + str(len(depop_likes_list)))
        print('depop bag list ' + str(len(depop_bag_list)))

def sqlite_database_for_depop(the_table_name, sold_or_inv):
    """ 
    creates our table in sqlite and inserts the data that we have created above
    """
    combined = zip(depop_title_list, depop_price_list, depop_likes_list, depop_bag_list, depop_boosted_list, depop_brand_list, depop_url_list, depop_men_or_wmn_list, depop_gen_category_list, depop_sub_category_list, depop_size_list, depop_condition_list, ['; '.join(x) for x in depop_images_list], depop_image_len_list, depop_color1_list, depop_color2_list)
    combined_list = list(combined)

    connection = sqlite3.connect('D:\\Selenium_python2\\Depop_database.db')
    cursor = connection.cursor()
    table_name = f'dp_{sold_or_inv}_{the_table_name}'
    # Create a new table with an auto-incrementing primary key
    
    #be able to compare from last post scan, also able to document when the last scan was completed
    # try:
    #     command_create = f"""
    #         DROP TABLE {table_name}
    #     """
    #     cursor.execute(command_create)
    # except:
    #     pass

    command_create = f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            dp_title TEXT UNIQUE,
            dp_price INT,
            dp_likes INT,
            dp_bag INT,
            dp_boosted INT,
            dp_brand TEXT,
            dp_url_list TEXT,
            dp_men_or_wmn TEXT,
            dp_gen_category TEXT,
            dp_category TEXT,
            dp_size TEXT,
            dp_condition TEXT,
            dp_images TEXT,
            dp_image_len INT,
            dp_color1 TEXT,
            dp_color2 TEXT DEFAULT ''
        )
    """
    cursor.execute(command_create)

    # Insert data into the new table
    command_insert = f"""
        INSERT OR IGNORE INTO {table_name} (
            dp_title,
            dp_price,
            dp_likes,
            dp_bag,
            dp_boosted,
            dp_brand,
            dp_url_list,
            dp_men_or_wmn,
            dp_gen_category,
            dp_category,
            dp_size,
            dp_condition,
            dp_images,
            dp_image_len,
            dp_color1,
            dp_color2
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """

    cursor.executemany(command_insert, combined_list)
    connection.commit()
    connection.close()

if __name__ == '__main__':
    try:
        start_up(driver, 'argelarroyo2001@gmail.com', 'Pineappleguy305')
        scroll_down_script(driver, 'Selling')
        each_post()
    except Exception as e:
        print(e)
        traceback.print_exc()
    checking_all_match()
    # sqlite_database_for_depop(account_name, 'inv')
    # os.system("shutdown /s /t 1")


# now just need a selenium auto scraper to  go through every single post, save the title to a list, compared the titles to each list, 
# maybe create a zipped file with enumeration as well to easily identify it easier, like in sql, title could be unique id
# actually enumaration might not be needed, since titles will just be compared
#can make a database, maybe using sql, to compared all listings from all platforms,
# if its sold on one platform, must be sold on all, so that would be good to like check it every day and if it is sold on one,
# runs program to list it as sold on other platforms.

# Once I am able to extract the text from the script tag, then I am able to get the rest of the info I need, including pics, the # of pics, price and name, then can start uploading all of my items into the sql database.

# Focus more on input, and output will come with it.

#issues currently is the scroll down script is having an issue where its not scrolling all the way down for some reason, maybe instead I can just use the load_more execute script to go until the button is pressable then
# press it with that, also the likes is messing up too, not giving me the value I need, and not sure on how to extract it either.
#still need to find a way to see how long this post has been up, but at the moment I cant seem to find a way, maybe I'll just look through the code on the page, and see if it is hidden somewhere there, bc it only gives me the
# last time it was relisted, and that is not accurate at all, overall this works great now, ready to enter into sql, but that is also something I will need to learn how to do as well.

#once i get this database working, then how else would I need to interact with it,
# moving sold posts, from depop_inventory to depop_sold
# creating a sold database to move it into so I dont just loose all the info

#error with hats, so I will need to fix it.  Becuase there are no size with hats, and must enter that as a blank,
#maybe I can assign it to be fixed by identifying if it is a hat, and if it is, rewrite the function to accomodate for that
# shoes are good, so its just an issue with items that don't need/have a size

#that issue has been fixed and is even better now with using the data-testid rather than the class attribute, so much more accurate and less code,
# Current Errors:
# trying to skip the meet the seller post with identifying the price of 99999.00, but im doing something wrong.  The price is staying 99999.00 for every other post, it is not refreshing, so maybe
# theres a piece of code im using in the function that restores everything, bc even after resetting the variable, it still stays at 99999.00, so will work on that tmrw

#script now works but since I need to  click on the page to open it up, maybe I can leave the og page as it, create new tab, extract, then close that 2nd page and continue with the first page.

#OK so i got it to open a new tab, but the url is off and is sending me a 404 error, need to figure out how to get the correct url for the post just off the page, 
#ok have figured out where I could find that url, then I would just have to replace "/products/realvintage_-" with 0 the I'll have the end of the link, which is the only dynamic part I will need to open a new tab with 
#the proper url, so tmrw I will need to figure out how to extract the href from the element, but so far so good, once I figure that out, then everything else will be cake and I can finally start creating a big database
#with all my posts.

# 3-10-23
# finally figured out the whole tab situation and now have this script almost ready to run, I want to make it better, I want to parse through the title and edit to add a brand if needed, since most of my objects dont contains a brand at all
# also be able to identify if this item is in someones bag as well, just as another factor I could use to get more info out of my products, like the like filter but better, since it being in the bag indicates someone wants to buy it

#small issue with meet the seller, I believe i fixed it, but need to double check, wasnt able to test it
# need to figure out the brand issue, identify whether or not there is a brand, fix that last else statement in that section and fix it
# then once i fix the brand issue, then I am all done, and can successfully start running the script and creating a depop database for all my listings, then have to do the same thing to grailed, then ebay maybe?
# ebay might already have an option to download all listings so I could just use that, regardless it is good practice,  this took a lot of work, maybe like 20-30 hrs overall, if i was to make an estimate, epecially this wweek
# I was only working on this project all week, so Im glad I am able to fix it correctly, yeah cause when jacob came on tuesday, i was working on it, and worked on it the day before as well

